---
layout: post
title: Recurrent Model of Visual Attention
comments: True
author: nicholas-leonard
---

<!---# Recurrent Model of Visual Attention-->

In this blog post, I want to discuss how we at Element Inc were able to
implemented the recurrent attention model (RAM) described in [[1]](#rmva.ref). 
Not only were we able to reproduce the paper, but we also made of bunch of
useful and usable code available in the process.

We decided that this paper was worth reproducing. 
Let's face it, many of the research papers are difficult or impossible to reproduce
even with help from the original authors [[2]]. 
The authors of [[1]] did a great job on making their paper clear, concise and relatively easy to reproduce.
All but two hyper-parameters are detailed, which means that hyper-parameter search would be relatively easy.
As for those two, the specifies that they were found through random-search.

The paper describes a RAM that can be applied to image classification datasets.
The model is designed in such a way that it has a bandwidth limited sensor of the input image.
So if the input image is of size `28x28` (height x width), the RAM may only be able to 
sense an area of size `8x8` at any given time. These small senses areas are refered to as *glimpses*. 

## SpatialGlimpse ##

Actually, the paper's glimpse sensor is little bit more 
complicated than what we described above, but nevertheless simple. 
Keeping in step with the modular spirit of [Torch7]'s [nn] package, we created a 
[SpatialGlimpse](https://github.com/nicholas-leonard/dpnn#nn.SpatialGlimpse) module.

```lua
module = nn.SpatialGlimpse(size, depth, scale)
```


Basically, if you give it input image is the classic `3x512x512` Lenna:

![lenna](images/lenna.png)

And you run this code on it:

```lua
require 'dpnn'
require 'image'

img = image.lena()
loc = torch.Tensor{0,0} -- 0.0 is the center of the image

sg = nn.SpatialGlimpse(64, 3, 2)

output = sg:forward{img, loc}
print(output:size()) --   9 x 64 x 64


-- unroll the glimpse onto its different depths
outputs = torch.chunk(output, 3)
display = image.toDisplayTensor(outputs)
image.save("glimpse-output.png", display)
```

You will end up with the following (unrolled) glimpse :

![glimpse](images/glimpse-output.png)

While the input is a `3x512x512` image (786432 scalars), 
the output is very small : `9x64x64` (36864 scalars), or about 
5% the size of the original image.
Since the glimse here has a `depth=3` (i.e. uses 3 patches) 
where each successive patch is `scale` times the size of the prevous one.
So we end up with a high-resolution patch of a small area, 
and successively lower-resolution (i.e. downscaled) patches of larger areas.
I love how this approximates our own human attention mechanism. 
While we can really see the details of what we focus our attention on,
we maintain a blurry view of the outskirts.

Although not necessary for reproducing this paper, 
the `SpatialGlimpse` can also be partially backpropagated through.
While the gradient w.r.t. the `img` tensor can be obtained, 
the gradients w.r.t. the `location` (i.e. the `x,y` coordinates of the glimpse) will be zero. 
This is because the glimpse operation cannot be differentiated w.r.t. the `location`.
Which brings us to the main difficulty of attention models : 
how can we teach the network to glimpse at the right `locations`?

## Attention ##

Some attention models use a fully differentiable attention mechanism, 
like the recent DRAW paper [[3]](#rmva.ref). 
But the RAM model uses a non-differenciable attention mechanism.
Specifically, it uses a the REINFORCE algorithm [[4]]. 
This algorithm allows one to train stochastic units through reinforcement learning.

### REINFORCE ###

The REINFORCE algorithm is very powerful as it can be 
used to optimize stochastic units (conditioned on and input),
such that they minimize an objective function.
Unlike backpropagation [[5]], this objective function doesn't need
to be differentiable.

The RAM model uses the REINFORCE algorithm to train the locator network :

```lua
-- actions (locator)
locator = nn.Sequential()
locator:add(nn.Linear(opt.hiddenSize, 2))
locator:add(nn.HardTanh()) -- bounds mean between -1 and 1
locator:add(nn.ReinforceNormal(2*opt.locatorStd)) -- sample from normal, uses REINFORCE learning rule
locator:add(nn.HardTanh()) -- bounds sample between -1 and 1
locator:add(nn.MulConstant(opt.unitPixels/ds:imageSize("h")))
```

The input is the previous recurrent hidden state `h[t-1]`. 
During training, the output is sampled from a normal 
distribution with fixed standard deviation.
The mean is conditioned on `h[t-1]` through an affine transform (`Linear`).
During evaluation, instead of sampling, we take the Maximum a Posteriori, i.e. the mean.

The [ReinforceNormal](https://github.com/nicholas-leonard/dpnn#nn.ReinforceNormal) module 
implements the REINFORCE algorithm for the normal distribution.
Unlike most `Modules`, `ReinforceNormal` ignores the `gradOutput` when `backward` is called.
This is because the units it embodies are in fact stochastic.
So then how does it generate `gradInputs` when the `backward` is called?
It uses the REINFORCE algorithm, which requires that a reward function be defined.
The reward uses by the paper is very simple, and yet not differentiable :
```
R = I(y=t)
```
where `R` is the raw reward, `I(x)` is `1` when `x` is true and `0` otherwise (indicator function), 
`y` is the predicted class, `t` is the target class. 
Or in Lua :
```lua
R = y==t and 1 or 0
```

The REINFORCE algorithm requires that we differentiate the 
probability density or mass function of the distribution
w.r.t. the parameters. So given the following variables :

  * `f` : normal probability density function
  * `x` : the sampled values (i.e. `ReinforceNormal.output`)
  * `u` : mean (`input` to `ReinforceNormal`)
  * `s` : standard deviation (`ReinforceNormal.stdev`)

the derivative of log normal w.r.t. mean `u` is :
```
d ln(f(x,u,s))   (x - u)
-------------- = -------
     d u           s^2
```


<a name='rmva.ref'></a>
## References

1. *Volodymyr Mnih, Nicolas Heess, Alex Graves, Koray Kavukcuoglu*, Recurrent Models of Visual Attention, [[NIPS 2014]](http://papers.nips.cc/paper/5542-recurrent-models-of-visual-attention)
2. **, (http://www.sciencemag.org/content/349/6251/aac4716)
3. DRAW
4. REINFORCE
5. Backprop
3. *Yoshua Bengio, Nicholas Leonard, Aaron Courville*. Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation [[arxiv]](http://arxiv.org/abs/1308.3432)
