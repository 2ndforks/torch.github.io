---
layout: post
title: Recurrent Model of Visual Attention
comments: True
author: nicholas-leonard
---

<!---# Recurrent Model of Visual Attention-->

In this blog post, I want to discuss how we at [Element-Research](http://dev.discoverelement.com/research) 
implemented the recurrent attention model (RAM) described in [[1]](#rmva.ref). 
Not only were we able to reproduce the paper, but we also made of bunch of
modular code available in the process. 
You can reproduce the RAM on the MNIST dataset using this 
[training script](https://github.com/Element-Research/rnn/blob/master/examples/recurrent-visual-attention.lua).
We will use snippets of that script throughout this post.

After reading the paper, we decided that it was worth reproducing. 
Let's face it, many of the research papers are difficult or impossible to reproduce
even with the help of the original authors [[2]]. 
However, the authors of [[1]] did a great job on making their paper clear, 
concise and relatively easy to reproduce.
All hyper-parameters but two are detailed, which means that hyper-parameter search would be relatively easy.
As for those two, the paper specifies that they were found through random-search.
So we found them through random search.

The paper describes a RAM that can be applied to image classification datasets.
The model is designed in such a way that it has a bandwidth limited sensor of the input image.
As an example, if the input image is of size `28x28` (height x width), 
the RAM may only be able to sense an area of size `8x8` at any given time-step. 
These small sensed areas are refered to as *glimpses*. 

## SpatialGlimpse ##

Actually, the paper's glimpse sensor is little bit more 
complicated than what we described above, but nevertheless simple. 
Keeping in step with the modular spirit of [Torch7]'s [nn] package, we created a 
[SpatialGlimpse](https://github.com/nicholas-leonard/dpnn#nn.SpatialGlimpse) module.

```lua
module = nn.SpatialGlimpse(size, depth, scale)
```

Basically, if you feed it an image, say the classic `3x512x512` Lenna image:

![lenna](images/lenna.png)

And you run this code on it:

```lua
require 'dpnn'
require 'image'

img = image.lena()
loc = torch.Tensor{0,0} -- 0.0 is the center of the image

sg = nn.SpatialGlimpse(64, 3, 2)

output = sg:forward{img, loc}
print(output:size()) --   9 x 64 x 64


-- unroll the glimpse onto its different depths
outputs = torch.chunk(output, 3)
display = image.toDisplayTensor(outputs)
image.save("glimpse-output.png", display)
```

You will end up with the following (unrolled) glimpse :

![glimpse](images/glimpse-output.png)

While the input is a `3x512x512` image (786432 scalars), 
the output is very small : `9x64x64` (36864 scalars), or about 
5% the size of the original image.
Since the glimse here has a `depth=3`, i.e. it uses 3 patches,
Each successive patch is `scale` times the size of the prevous one.
So we end up with a high-resolution patch of a small area, 
and successively lower-resolution (i.e. downscaled) patches of larger areas.
I love how this approximates our own human attention mechanism. 
While we humans can really see the details of what we focus our attention on,
we nevertheless maintain a blurry view of the outskirts.

Although not necessary for reproducing this paper, 
the `SpatialGlimpse` can also be partially backpropagated through.
While the gradient w.r.t. the `img` tensor can be obtained, 
the gradients w.r.t. the `location` (i.e. the `x,y` coordinates of the glimpse) will be zero. 
This is because the glimpse operation cannot be differentiated w.r.t. the `location`.
Which brings us to the main difficulty of attention models : 
how can we teach the network to glimpse at the right `locations`?

## REINFORCE algorithm ##

Some attention models use a fully differentiable attention mechanism, 
like the recent DRAW paper [[3]](#rmva.ref). 
But the RAM model uses a non-differenciable attention mechanism.
Specifically, it uses a the REINFORCE algorithm [[4]]. 
This algorithm allows one to train stochastic units through reinforcement learning.

The REINFORCE algorithm is very powerful as it can be 
used to optimize stochastic units (conditioned on an input),
such that they minimize an objective function (i.e. a reward function).
*Unlike backpropagation [[5]], this objective doesn't need to be differentiable*.

The RAM model uses the REINFORCE algorithm to train the `locator` network :

```lua
-- actions (locator)
locator = nn.Sequential()
locator:add(nn.Linear(opt.hiddenSize, 2))
locator:add(nn.HardTanh()) -- bounds mean between -1 and 1
locator:add(nn.ReinforceNormal(2*opt.locatorStd)) -- sample from normal, uses REINFORCE learning rule
locator:add(nn.HardTanh()) -- bounds sample between -1 and 1
locator:add(nn.MulConstant(opt.unitPixels/ds:imageSize("h")))
```

The input is the previous recurrent hidden state `h[t-1]`. 
During training, the output is sampled from a normal 
distribution with fixed standard deviation.
The mean is conditioned on `h[t-1]` through an affine transform (i.e. `Linear` module).
During evaluation, instead of sampling from the distribution, the output is taken to be the input, i.e. the mean.

The [ReinforceNormal](https://github.com/nicholas-leonard/dpnn#nn.ReinforceNormal) module 
implements the REINFORCE algorithm for the normal distribution.
Unlike most `Modules`, `ReinforceNormal` ignores the `gradOutput` when `backward` is called.
This is because the units it embodies are in fact stochastic.
So then how does it generate `gradInputs` when the `backward` is called?
It uses the REINFORCE algorithm, which requires that a reward function be defined.
The reward used by the paper is very simple, and yet not differentiable (eq. 1) :
```
R = I(y=t)
```
where `R` is the raw reward, `I(x)` is `1` when `x` is true and `0` otherwise 
(see [indicator function](https://en.wikipedia.org/wiki/Indicator_function)), 
`y` is the predicted class, `t` is the target class. 
Or in Lua :

```lua
R = y==t and 1 or 0
```

The REINFORCE algorithm requires that we differentiate the 
probability density or mass function (PDF/PMF) of the distribution
w.r.t. the parameters. So given the following variables :

  * `f` : normal probability density function
  * `x` : the sampled values (i.e. `ReinforceNormal.output`)
  * `u` : mean (`input` to `ReinforceNormal`)
  * `s` : standard deviation (`ReinforceNormal.stdev`)

the derivative of log normal w.r.t. mean `u` is :
```
d ln(f(x,u))   (x - u)
------------ = -------
     d u         s^2
```

So where does `d ln(f(x,u,s) / d u` fit in with the reward? 
Well, in order to obtain a gradient of the reward `R` w.r.t.
to input `u`, you apply the following equation (eq. 2, i.e. the REINFORCE algorithm):
```
d R                 d ln(f(x,u)) 
--- = a * (R - b) * --------------
d u                      d u 
```
where 
 
  * `a` (alpha) is just a scaling factor like the learning rate ; and
  * `b` is a baseline reward used to reduce the variance of the gradient. 
  * `f` is the PMF/PDF
  * `u` is the parameter w.r.t. which you want to get a gradient
  * `x` is the sampled value.
  
In the paper, they take `b` to be the expected reward `E[R]`. 
They approximate the expectation by making `b` a parameter of the model .
For each example, the mean square error between `R` and `b` is minimized by 
backpropagation. The beauty of doing this, instead of 
say making `b` a moving average of `R`, is that the baseline reward `b`
learns at the same rate as the rest of the model. 

We decided to implement the `reward = a * (R - b)` part of eq. 2
within a [VRClassReward](https://github.com/nicholas-leonard/dpnn/blob/master/README.md#nn.VRClassReward) criterion,
which also implements the paper's varianced-reduced classification reward function (eq. 1):

```lua
vcr = nn.VRClassReward(module [, scale, criterion])
```

The nn package was primarily built for backpropagation, so we had to find a not-too-hacky way of
broadcasting the `reward` to the different 
[Reinforce](https://github.com/nicholas-leonard/dpnn/blob/master/README.md#nn.Reinforce) modules.
We did this by having the criterion take the `module` as argument,
and adding the [Module:reinforce(reward)](https://github.com/nicholas-leonard/dpnn/blob/master/README.md#nn.Module.reinforce)
method. The latter allows the `Reinforce` modules (like `ReinforceNormal`) to 
hold on to the criterion's broadcasted `reward` for later. Later being when `backward` is called 
and the `Reinforce` module(s) computes the `gradInput` 
(i.e. `d R / d U`) using the REINFORCE algorithm (eq. 2).
And then `nn` is happy, because with the `gradInput`, it can continue 
the backpropagation from the `Reinforce` module to its predecessors.

So to summarize, if you can differentiate the PMF/PDF w.r.t. to its parameters,
then you can use the REINFORCE algorithm on it. We have already implemented 
modules for the categorical and bernoulli distributions :
   
   * [ReinforceCategorical](https://github.com/nicholas-leonard/dpnn/blob/master/README.md#nn.ReinforceCategorical)
   * [ReinforceBernoulli](https://github.com/nicholas-leonard/dpnn/blob/master/README.md#nn.ReinforceBernoulli)

## Recurrent Attention Model 

Okay, so we discussed the glimpse module and the REINFORCE algorithm, 
lets talk about the recurrent attention model. We can divide the model 
into its respective components :

The location sensor. Its input is `x`, `y` coordinates of the current glimpse location, 
so the network knows *where* its looking at each time-step :
```lua
locationSensor = nn.Sequential()
locationSensor:add(nn.SelectTable(2))
locationSensor:add(nn.Linear(2, opt.locatorHiddenSize))
locationSensor:add(nn[opt.transfer]())
```

The glimpse sensor is *what* it's looking at :

```lua
glimpseSensor = nn.Sequential()
glimpseSensor:add(nn.DontCast(nn.SpatialGlimpse(opt.glimpsePatchSize, opt.glimpseDepth, opt.glimpseScale):float(),true))
glimpseSensor:add(nn.Collapse(3))
glimpseSensor:add(nn.Linear(ds:imageSize('c')*(opt.glimpsePatchSize^2)*opt.glimpseDepth, opt.glimpseHiddenSize))
glimpseSensor:add(nn[opt.transfer]())
```

The glimpse network is where the glimpse and location sensor mix through 
a hidden layer to form the input layer of the Recurrent Neural Network (RNN) :

```lua
glimpse = nn.Sequential()
glimpse:add(nn.ConcatTable():add(locationSensor):add(glimpseSensor))
glimpse:add(nn.JoinTable(1,1))
glimpse:add(nn.Linear(opt.glimpseHiddenSize+opt.locatorHiddenSize, opt.imageHiddenSize))
glimpse:add(nn[opt.transfer]())
glimpse:add(nn.Linear(opt.imageHiddenSize, opt.hiddenSize))
```

The RNN is the where the glimpse network and the recurrent layer come together.
The output of the `rnn` module is the hidden state `h[t]` where `t` indexes the time-step.
We used the [rnn](https://github.com/Element-Research/rnn) package to build it :

```lua 
-- rnn recurrent layer
recurrent = nn.Linear(opt.hiddenSize, opt.hiddenSize)

-- recurrent neural network
rnn = nn.Recurrent(opt.hiddenSize, glimpse, recurrent, nn[opt.transfer](), 99999)
```

We have already seen the locator network above, but here it is again :

```lua
-- actions (locator)
locator = nn.Sequential()
locator:add(nn.Linear(opt.hiddenSize, 2))
locator:add(nn.HardTanh()) -- bounds mean between -1 and 1
locator:add(nn.ReinforceNormal(2*opt.locatorStd)) -- sample from normal, uses REINFORCE learning rule
locator:add(nn.HardTanh()) -- bounds sample between -1 and 1
locator:add(nn.MulConstant(opt.unitPixels/ds:imageSize("h")))
```

It is responsible to sample the next location `l[t]` given the previous hidden state `h[t-1]`.
We should however bring your attention to the `opt.unitPixels` variable, 
as it is very important and not specified in the paper. 
This variable basically outlines how far (in pixels) near the borders the 
center of each glimpse can reach with respect to the center. 
So a value of 12 (the default) means that the center of the glimpse can 
be anywhere between the 3rd and 26th pixel. So glimpses of the corner 
will have fewer zero-padding values then if `opt.unitPixels = 14`.

A special thanks to [Jimmy Ba](https://www.facebook.com/lei.jba) for helping us figure this out.
He worked on the original paper when doing his intership at Google Deep Mind,
and extended the RAM to multiple-object detection [[6]].

We to encapsulate the `rnn` and `locator` into a module that would 
capture the essence of the RAM model. 
So we decided to implement a module that took the full image as input, 
and output the sequence of hidden states `h`. 
As its name implies, this module is a general purpose 
[RecurrentAttention](https://github.com/Element-Research/rnn#rnn.RecurrentAttention) module :

```lua
attention = nn.RecurrentAttention(rnn, locator, opt.rho, {opt.hiddenSize})
```

It's so general-purpose that you can use it with an 
[LSTM](https://github.com/Element-Research/rnn#rnn.LSTM) module,
and different glimpse or locator modules. As long as these respectively preserve 
the same `{input, action} -> output` interfaces (glimpse) and use the REINFORCE algorithm (locator), of course.

Next we build an agent by stacking a classifier on top of the `RecurrentAttention` module.
The classifier's input is the last hidden state `h[T]`, 
where `T` is the total number of time-steps (i.e. glimpses to take):

```lua
-- model is a reinforcement learning agent
agent = nn.Sequential()
agent:add(nn.Convert(ds:ioShapes(), 'bchw'))
agent:add(attention)

-- classifier :
agent:add(nn.SelectTable(-1))
agent:add(nn.Linear(opt.hiddenSize, #ds:classes()))
agent:add(nn.LogSoftMax())
```

You might recall that the REINFORCE algorithm requires a baseline reward `b`?
This is where that happens (yes, it requires a little bit of `nn` kung-fu):
```
-- add the baseline reward predictor
seq = nn.Sequential()
seq:add(nn.Constant(1,1))
seq:add(nn.Add(1))
concat = nn.ConcatTable():add(nn.Identity()):add(seq)
concat2 = nn.ConcatTable():add(nn.Identity()):add(concat)

-- output will be : {classpred, {classpred, basereward}}
agent:add(concat2)
```

At this point, the model is ready for training. 
The `agent` instance is effectively the RAM model detailed in the paper. 

## Training 

Training is pretty straight-forward after that. 
The script can be launched with the default hyper-parameters.
It should take between 5 and 10 runs to find a validation minima that 
reproduces the MNIST results of the paper.

<a name='rmva.ref'></a>
## References

1. *Volodymyr Mnih, Nicolas Heess, Alex Graves, Koray Kavukcuoglu*, Recurrent Models of Visual Attention, [[NIPS 2014]](http://papers.nips.cc/paper/5542-recurrent-models-of-visual-attention)
2. **, (http://www.sciencemag.org/content/349/6251/aac4716)
3. DRAW
4. REINFORCE
5. Backprop
6. Jimmy Ba
